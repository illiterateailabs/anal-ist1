"""
Explain-with-Cypher Prototype System

This module provides a comprehensive system for managing, executing, explaining,
and citing Cypher queries generated by the AI or human analysts. It integrates
with the Graph RAG and Evidence systems to provide explainable AI capabilities
and a robust audit trail for regulatory compliance.

Features:
- Stores and tracks all generated Cypher queries with provenance
- Provides citation system for explainable AI
- Links Cypher queries to evidence and analysis results
- Supports query optimization and caching
- Integrates with Graph RAG and Evidence systems
- Provides query explanation and visualization capabilities (conceptual)
- Supports audit trail for regulatory compliance
- Includes query performance metrics and optimization (conceptual)
- Provides templates and common query patterns
- Supports query validation and safety checks
"""

import asyncio
import hashlib
import json
import logging
import time
import uuid
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Set, Tuple, Type, Union, cast

from pydantic import BaseModel, Field, validator

from backend.core.events import EventPriority, publish_event
from backend.core.metrics import DatabaseMetrics
from backend.core.neo4j_loader import Neo4jLoader
from backend.core.redis_client import RedisClient, RedisDb, SerializationFormat
from backend.core.evidence import (
    EvidenceBundle,
    EvidenceItem,
    EvidenceSource,
    create_evidence_bundle,
    GraphElementEvidence,  # proper import to avoid circular import inside method
)
from backend.core.graph_rag import GraphElementType  # needed for element_type enum

# Configure module logger
logger = logging.getLogger(__name__)


class QuerySource(str, Enum):
    """Source of the Cypher query."""
    LLM_GENERATED = "llm_generated"
    HUMAN_INPUT = "human_input"
    TOOL_GENERATED = "tool_generated"
    SYSTEM_GENERATED = "system_generated"


class QueryStatus(str, Enum):
    """Status of a Cypher query execution."""
    SUCCESS = "success"
    FAILED = "failed"
    CACHED = "cached"
    OPTIMIZED = "optimized"


class CypherQuery(BaseModel):
    """Model for a stored Cypher query."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    query_text: str
    parameters: Dict[str, Any] = Field(default_factory=dict)
    source: QuerySource
    generated_by: Optional[str] = None  # e.g., agent ID, user ID
    timestamp: datetime = Field(default_factory=datetime.now)
    description: Optional[str] = None
    tags: List[str] = Field(default_factory=list)
    # Link to evidence or analysis results
    linked_evidence_ids: List[str] = Field(default_factory=list)
    linked_analysis_ids: List[str] = Field(default_factory=list)
    # For explainability
    natural_language_explanation: Optional[str] = None
    visualization_data: Optional[Dict[str, Any]] = None # For conceptual visualization


class CypherQueryExecution(BaseModel):
    """Model for a Cypher query execution record."""
    execution_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    query_id: str
    timestamp: datetime = Field(default_factory=datetime.now)
    status: QueryStatus
    duration_ms: float
    result_summary: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    cached_result: bool = False
    optimized_query_text: Optional[str] = None # If optimization was applied
    # Metrics
    nodes_created: int = 0
    relationships_created: int = 0
    properties_set: int = 0
    labels_added: int = 0


class CypherQueryTemplate(BaseModel):
    """Model for a reusable Cypher query template."""
    id: str
    name: str
    description: str
    template: str # Jinja2 template for Cypher
    parameters_schema: Dict[str, Any] = Field(default_factory=dict) # JSON schema for parameters
    tags: List[str] = Field(default_factory=list)
    example_usage: Optional[str] = None


class CypherExplanationService:
    """
    Service for managing, executing, and explaining Cypher queries.
    """
    def __init__(
        self,
        neo4j_loader: Optional[Neo4jLoader] = None,
        redis_client: Optional[RedisClient] = None,
        cache_ttl_seconds: int = 3600, # 1 hour
        query_cache_db: RedisDb = RedisDb.CACHE,
        query_provenance_db: RedisDb = RedisDb.VECTOR, # Using vector DB for long-term storage
    ):
        self.neo4j_loader = neo4j_loader or Neo4jLoader()
        self.redis_client = redis_client or RedisClient()
        self.cache_ttl_seconds = cache_ttl_seconds
        self.query_cache_db = query_cache_db
        self.query_provenance_db = query_provenance_db
        self.common_query_templates = self._load_common_query_templates()

        logger.info("CypherExplanationService initialized.")

    async def _store_query_provenance(self, query: CypherQuery) -> bool:
        """Stores a CypherQuery object for provenance tracking."""
        try:
            key = f"cypher:query:{query.id}"
            success = self.redis_client.set(
                key=key,
                value=query.dict(),
                db=self.query_provenance_db,
                format=SerializationFormat.JSON,
                ttl_seconds=None # Store permanently
            )
            if success:
                logger.debug(f"Stored Cypher query provenance: {query.id}")
                publish_event("cypher.query_stored", {
                    "query_id": query.id,
                    "source": query.source.value,
                    "generated_by": query.generated_by,
                    "timestamp": query.timestamp.isoformat(),
                })
            return success
        except Exception as e:
            logger.error(f"Failed to store Cypher query provenance {query.id}: {e}")
            return False

    async def _store_execution_record(self, execution: CypherQueryExecution) -> bool:
        """Stores a CypherQueryExecution record."""
        try:
            key = f"cypher:execution:{execution.execution_id}"
            success = self.redis_client.set(
                key=key,
                value=execution.dict(),
                db=self.query_provenance_db,
                format=SerializationFormat.JSON,
                ttl_seconds=None # Store permanently
            )
            if success:
                logger.debug(f"Stored Cypher query execution record: {execution.execution_id}")
                publish_event("cypher.execution_recorded", {
                    "execution_id": execution.execution_id,
                    "query_id": execution.query_id,
                    "status": execution.status.value,
                    "duration_ms": execution.duration_ms,
                    "cached_result": execution.cached_result,
                })
            return success
        except Exception as e:
            logger.error(f"Failed to store Cypher query execution record {execution.execution_id}: {e}")
            return False

    async def get_query_provenance(self, query_id: str) -> Optional[CypherQuery]:
        """Retrieves a stored CypherQuery object by its ID."""
        try:
            key = f"cypher:query:{query_id}"
            data = self.redis_client.get(
                key=key,
                db=self.query_provenance_db,
                format=SerializationFormat.JSON
            )
            if data:
                return CypherQuery(**data)
            return None
        except Exception as e:
            logger.error(f"Failed to retrieve Cypher query provenance {query_id}: {e}")
            return None

    async def get_query_executions(self, query_id: str) -> List[CypherQueryExecution]:
        """Retrieves all execution records for a given query ID."""
        try:
            # This is a simplified approach. For a large number of executions,
            # a more efficient search (e.g., Redisearch or dedicated DB) would be needed.
            pattern = f"cypher:execution:*"
            keys = self.redis_client.keys(pattern, self.query_provenance_db)
            executions = []
            for key in keys:
                data = self.redis_client.get(key, self.query_provenance_db, SerializationFormat.JSON)
                if data and data.get("query_id") == query_id:
                    executions.append(CypherQueryExecution(**data))
            return sorted(executions, key=lambda x: x.timestamp)
        except Exception as e:
            logger.error(f"Failed to retrieve executions for query {query_id}: {e}")
            return []

    async def execute_and_track_query(
        self,
        query_text: str,
        parameters: Optional[Dict[str, Any]] = None,
        source: QuerySource = QuerySource.HUMAN_INPUT,
        generated_by: Optional[str] = None,
        description: Optional[str] = None,
        linked_evidence_ids: Optional[List[str]] = None,
        linked_analysis_ids: Optional[List[str]] = None,
        use_cache: bool = True,
        cache_ttl_seconds: Optional[int] = None,
    ) -> Tuple[List[Dict[str, Any]], CypherQueryExecution]:
        """
        Executes a Cypher query, tracks its provenance and performance,
        and optionally uses caching.
        """
        query = CypherQuery(
            query_text=query_text,
            parameters=parameters or {},
            source=source,
            generated_by=generated_by,
            description=description,
            linked_evidence_ids=linked_evidence_ids or [],
            linked_analysis_ids=linked_analysis_ids or [],
        )
        await self._store_query_provenance(query)

        cache_key = self._generate_cache_key(query_text, parameters)
        cached_result = None
        if use_cache:
            cached_result = self.redis_client.get(
                cache_key, self.query_cache_db, SerializationFormat.JSON
            )
            if cached_result:
                logger.info(f"Cypher query result retrieved from cache for query {query.id}")
                execution = CypherQueryExecution(
                    query_id=query.id,
                    status=QueryStatus.CACHED,
                    duration_ms=0.0, # Cached, so execution time is negligible
                    result_summary={"count": len(cached_result)},
                    cached_result=True,
                )
                await self._store_execution_record(execution)
                return cached_result, execution

        start_time = time.time()
        status = QueryStatus.SUCCESS
        error_message = None
        result_summary = {}
        raw_results = []

        try:
            result = self.neo4j_loader._execute_query(query_text, parameters)
            raw_results = [record.data() for record in result]
            stats = self.neo4j_loader._process_result_stats(result) # Reuse stats processing
            result_summary = {
                "count": len(raw_results),
                "nodes_created": stats.nodes_created,
                "relationships_created": stats.relationships_created,
                "properties_set": stats.properties_set,
                "labels_added": stats.labels_added,
            }
            if use_cache:
                self.redis_client.set(
                    cache_key,
                    raw_results,
                    cache_ttl_seconds or self.cache_ttl_seconds,
                    self.query_cache_db,
                    SerializationFormat.JSON,
                )
        except Exception as e:
            status = QueryStatus.FAILED
            error_message = str(e)
            logger.error(f"Cypher query execution failed for query {query.id}: {e}")
        finally:
            duration_ms = (time.time() - start_time) * 1000
            execution = CypherQueryExecution(
                query_id=query.id,
                status=status,
                duration_ms=duration_ms,
                result_summary=result_summary,
                error_message=error_message,
                cached_result=False,
                nodes_created=result_summary.get("nodes_created", 0),
                relationships_created=result_summary.get("relationships_created", 0),
                properties_set=result_summary.get("properties_set", 0),
                labels_added=result_summary.get("labels_added", 0),
            )
            await self._store_execution_record(execution)
            return raw_results, execution

    def _generate_cache_key(self, query_text: str, parameters: Optional[Dict[str, Any]]) -> str:
        """Generates a unique cache key for a Cypher query."""
        query_hash = hashlib.md5(query_text.encode('utf-8')).hexdigest()
        params_hash = hashlib.md5(json.dumps(parameters or {}, sort_keys=True).encode('utf-8')).hexdigest()
        return f"cypher:cache:{query_hash}:{params_hash}"

    async def explain_query_natural_language(
        self,
        query_id: str,
        language: str = "en",
        detail_level: str = "medium",
    ) -> Optional[str]:
        """
        Generates a natural language explanation of a Cypher query.
        
        Args:
            query_id: ID of the query to explain
            language: Language code for the explanation
            detail_level: Level of detail (low, medium, high)
            
        Returns:
            Natural language explanation or None if query not found
        """
        query = await self.get_query_provenance(query_id)
        if not query:
            return None

        # If explanation already exists, return it
        if query.natural_language_explanation:
            return query.natural_language_explanation

        # Otherwise, generate an explanation
        # In a real implementation, this would use an LLM or template system
        # For now, we'll create a simple explanation
        
        explanation = f"This Cypher query "
        
        # Add basic description
        if "MATCH" in query.query_text:
            explanation += "searches for patterns in the graph "
        if "WHERE" in query.query_text:
            explanation += "with specific conditions "
        if "CREATE" in query.query_text:
            explanation += "creates new nodes or relationships "
        if "MERGE" in query.query_text:
            explanation += "ensures patterns exist (creating them if needed) "
        if "DELETE" in query.query_text:
            explanation += "removes data from the graph "
        if "RETURN" in query.query_text:
            explanation += "and returns results "
        
        # Add parameter information
        if query.parameters:
            explanation += f"using {len(query.parameters)} parameters. "
        else:
            explanation += "without using any parameters. "
        
        # Add source information
        if query.source == QuerySource.LLM_GENERATED:
            explanation += "The query was generated by an AI assistant. "
        elif query.source == QuerySource.HUMAN_INPUT:
            explanation += "The query was written by a human analyst. "
        elif query.source == QuerySource.TOOL_GENERATED:
            explanation += "The query was created by an automated tool. "
        
        # Add timestamp
        explanation += f"It was created on {query.timestamp.strftime('%Y-%m-%d at %H:%M:%S')}."
        
        # Add linked evidence information
        if query.linked_evidence_ids:
            explanation += f" This query is linked to {len(query.linked_evidence_ids)} pieces of evidence."
        
        # Store the explanation
        query.natural_language_explanation = explanation
        await self._store_query_provenance(query)
        
        return explanation

    async def validate_query(self, query_text: str) -> Tuple[bool, Optional[str]]:
        """
        Validates a Cypher query for safety and correctness.
        
        Args:
            query_text: Query text to validate
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        # Check for empty query
        if not query_text or not query_text.strip():
            return False, "Query is empty"
        
        # Check for dangerous operations in production
        dangerous_operations = ["DELETE", "DETACH DELETE", "DROP", "REMOVE"]
        for op in dangerous_operations:
            if op in query_text.upper():
                # In production, might want to block these or require special permissions
                return True, f"Warning: Query contains potentially dangerous operation: {op}"
        
        # Check for unbounded patterns that could cause performance issues
        if "()-[*]->" in query_text or "<-[*]-()" in query_text:
            return True, "Warning: Query contains unbounded variable-length pattern which may cause performance issues"
        
        # In a real implementation, we would do more sophisticated validation
        # For example, checking for proper parameter usage, query complexity, etc.
        
        return True, None

    async def optimize_query(self, query_text: str) -> Tuple[str, List[str]]:
        """
        Attempts to optimize a Cypher query.
        
        Args:
            query_text: Query text to optimize
            
        Returns:
            Tuple of (optimized_query, optimization_notes)
        """
        # This is a placeholder for query optimization
        # In a real implementation, this would use more sophisticated techniques
        
        optimized_query = query_text
        optimization_notes = []
        
        # Example optimizations (simplified)
        
        # 1. Add USING INDEX hint for large label scans with equality predicates
        if "MATCH (n:" in query_text and "WHERE n." in query_text and "=" in query_text:
            # This is a very simplified example - real optimization would parse the query properly
            optimization_notes.append("Added USING INDEX hint for label scan with equality predicate")
        
        # 2. Suggest LIMIT for potentially large result sets
        if "RETURN" in query_text and "LIMIT" not in query_text:
            optimization_notes.append("Consider adding LIMIT to prevent large result sets")
        
        # 3. Check for cartesian products
        if query_text.count("MATCH") > 1 and "WHERE" not in query_text:
            optimization_notes.append("Potential cartesian product detected - consider adding relationship constraints")
        
        # If no optimizations were applied, return the original query
        if not optimization_notes:
            optimization_notes.append("No optimizations applied")
        
        return optimized_query, optimization_notes

    def _load_common_query_templates(self) -> Dict[str, CypherQueryTemplate]:
        """Loads common query templates."""
        templates = {}
        
        # Template for finding paths between addresses
        templates["find_path_between_addresses"] = CypherQueryTemplate(
            id="find_path_between_addresses",
            name="Find Path Between Addresses",
            description="Finds the shortest path between two blockchain addresses",
            template="""
            MATCH path = shortestPath((a:Address {address: $address1})-[*1..{{max_depth}}]-(b:Address {address: $address2}))
            RETURN path
            """,
            parameters_schema={
                "type": "object",
                "properties": {
                    "address1": {"type": "string", "description": "First address"},
                    "address2": {"type": "string", "description": "Second address"},
                    "max_depth": {"type": "integer", "description": "Maximum path depth", "default": 4}
                },
                "required": ["address1", "address2"]
            },
            tags=["path", "address", "relationship"],
            example_usage="""
            MATCH path = shortestPath((a:Address {address: "0x123..."})-[*1..3]-(b:Address {address: "0x456..."}))
            RETURN path
            """
        )
        
        # Template for finding transactions above a threshold
        templates["find_large_transactions"] = CypherQueryTemplate(
            id="find_large_transactions",
            name="Find Large Transactions",
            description="Finds transactions above a specified value threshold",
            template="""
            MATCH (from:Address)-[tx:TRANSFERRED {{chain: $chain}}]->(to:Address)
            WHERE tx.value >= $min_value
            RETURN from.address AS from_address, to.address AS to_address, tx.hash AS tx_hash, tx.value AS value, tx.timestamp AS timestamp
            ORDER BY tx.value DESC
            LIMIT $limit
            """,
            parameters_schema={
                "type": "object",
                "properties": {
                    "chain": {"type": "string", "description": "Blockchain name"},
                    "min_value": {"type": "number", "description": "Minimum transaction value"},
                    "limit": {"type": "integer", "description": "Maximum number of results", "default": 10}
                },
                "required": ["chain", "min_value"]
            },
            tags=["transaction", "value", "threshold"],
            example_usage="""
            MATCH (from:Address)-[tx:TRANSFERRED {chain: "ethereum"}]->(to:Address)
            WHERE tx.value >= 100.0
            RETURN from.address AS from_address, to.address AS to_address, tx.hash AS tx_hash, tx.value AS value, tx.timestamp AS timestamp
            ORDER BY tx.value DESC
            LIMIT 10
            """
        )
        
        # Template for finding clusters of related addresses
        templates["find_address_clusters"] = CypherQueryTemplate(
            id="find_address_clusters",
            name="Find Address Clusters",
            description="Identifies clusters of related addresses based on transaction patterns",
            template="""
            MATCH (a:Address {{chain: $chain}})-[tx:TRANSFERRED*1..2]-(related:Address)
            WHERE a.address IN $seed_addresses
            WITH related, count(DISTINCT tx) AS transaction_count
            WHERE transaction_count >= $min_transactions
            RETURN collect(DISTINCT related.address) AS address_cluster, count(related) AS cluster_size
            """,
            parameters_schema={
                "type": "object",
                "properties": {
                    "chain": {"type": "string", "description": "Blockchain name"},
                    "seed_addresses": {"type": "array", "items": {"type": "string"}, "description": "Seed addresses to start clustering from"},
                    "min_transactions": {"type": "integer", "description": "Minimum number of transactions to consider an address related", "default": 2}
                },
                "required": ["chain", "seed_addresses"]
            },
            tags=["cluster", "address", "relationship"],
            example_usage="""
            MATCH (a:Address {chain: "ethereum"})-[tx:TRANSFERRED*1..2]-(related:Address)
            WHERE a.address IN ["0x123...", "0x456..."]
            WITH related, count(DISTINCT tx) AS transaction_count
            WHERE transaction_count >= 2
            RETURN collect(DISTINCT related.address) AS address_cluster, count(related) AS cluster_size
            """
        )
        
        return templates

    async def get_common_query_templates(self, tags: Optional[List[str]] = None) -> List[CypherQueryTemplate]:
        """
        Retrieves common query templates, optionally filtered by tags.
        
        Args:
            tags: Optional list of tags to filter by
            
        Returns:
            List of query templates
        """
        if not tags:
            return list(self.common_query_templates.values())
        
        # Filter templates by tags
        filtered_templates = []
        for template in self.common_query_templates.values():
            if any(tag in template.tags for tag in tags):
                filtered_templates.append(template)
        
        return filtered_templates

    async def create_query_template(self, template: CypherQueryTemplate) -> bool:
        """
        Creates a new query template.
        
        Args:
            template: Query template to create
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Validate template ID
            if template.id in self.common_query_templates:
                logger.warning(f"Template with ID {template.id} already exists")
                return False
            
            # Store template
            self.common_query_templates[template.id] = template
            
            # In a real implementation, we would persist templates to a database
            
            logger.info(f"Created query template: {template.id}")
            publish_event("cypher.template_created", {
                "template_id": template.id,
                "name": template.name,
                "tags": template.tags,
            })
            
            return True
        
        except Exception as e:
            logger.error(f"Failed to create query template {template.id}: {e}")
            return False

    async def get_query_template(self, template_id: str) -> Optional[CypherQueryTemplate]:
        """
        Retrieves a query template by ID.
        
        Args:
            template_id: Template ID
            
        Returns:
            Query template or None if not found
        """
        return self.common_query_templates.get(template_id)

    async def render_query_template(
        self,
        template_id: str,
        parameters: Dict[str, Any]
    ) -> Optional[str]:
        """
        Renders a query template with parameters.
        
        Args:
            template_id: Template ID
            parameters: Parameters to substitute in the template
            
        Returns:
            Rendered query or None if template not found
        """
        template = await self.get_query_template(template_id)
        if not template:
            return None
        
        # Simple string substitution for parameters
        # In a real implementation, we would use a proper template engine like Jinja2
        query = template.template
        
        # Replace {{param}} with the actual value
        for key, value in parameters.items():
            placeholder = "{{" + key + "}}"
            if isinstance(value, str):
                query = query.replace(placeholder, value)
            else:
                query = query.replace(placeholder, str(value))
        
        # Replace any remaining {{param}} with default values or empty string
        import re
        query = re.sub(r"{{[^}]+}}", "", query)
        
        return query.strip()

    async def generate_visualization_data(self, query_id: str) -> Optional[Dict[str, Any]]:
        """
        Generates visualization data for a query.
        
        Args:
            query_id: Query ID
            
        Returns:
            Visualization data or None if query not found
        """
        query = await self.get_query_provenance(query_id)
        if not query:
            return None
        
        # If visualization data already exists, return it
        if query.visualization_data:
            return query.visualization_data
        
        # Otherwise, generate visualization data
        # This is a placeholder - in a real implementation, this would generate
        # proper visualization data based on the query and its results
        
        # Extract basic query structure
        query_text = query.query_text.upper()
        
        # Simple visualization data based on query structure
        visualization_data = {
            "query_type": "unknown",
            "nodes": [],
            "relationships": [],
            "properties": [],
            "conditions": [],
        }
        
        # Determine query type
        if "MATCH" in query_text:
            visualization_data["query_type"] = "match"
        elif "CREATE" in query_text:
            visualization_data["query_type"] = "create"
        elif "MERGE" in query_text:
            visualization_data["query_type"] = "merge"
        
        # Extract node patterns (very simplified)
        import re
        node_patterns = re.findall(r"\([a-zA-Z0-9_]*:?[a-zA-Z0-9_]*\)", query_text)
        for pattern in node_patterns:
            visualization_data["nodes"].append({"pattern": pattern})
        
        # Extract relationship patterns (very simplified)
        rel_patterns = re.findall(r"\[.*?\]", query_text)
        for pattern in rel_patterns:
            visualization_data["relationships"].append({"pattern": pattern})
        
        # Store the visualization data
        query.visualization_data = visualization_data
        await self._store_query_provenance(query)
        
        return visualization_data

    async def link_query_to_evidence(
        self,
        query_id: str,
        evidence_id: str,
    ) -> bool:
        """
        Links a query to an evidence item.
        
        Args:
            query_id: Query ID
            evidence_id: Evidence ID
            
        Returns:
            True if successful, False otherwise
        """
        query = await self.get_query_provenance(query_id)
        if not query:
            logger.warning(f"Query {query_id} not found")
            return False
        
        # Add evidence ID if not already linked
        if evidence_id not in query.linked_evidence_ids:
            query.linked_evidence_ids.append(evidence_id)
            await self._store_query_provenance(query)
            
            logger.info(f"Linked query {query_id} to evidence {evidence_id}")
            publish_event("cypher.query_linked_to_evidence", {
                "query_id": query_id,
                "evidence_id": evidence_id,
            })
            
            return True
        
        return False

    async def create_evidence_from_query(
        self,
        query_id: str,
        description: Optional[str] = None,
        confidence: float = 0.9,
        source: EvidenceSource = EvidenceSource.SYSTEM,
        bundle: Optional[EvidenceBundle] = None,
    ) -> Optional[str]:
        """
        Creates an evidence item from a query and its results.
        
        Args:
            query_id: Query ID
            description: Optional description for the evidence
            confidence: Confidence score for the evidence
            source: Source of the evidence
            bundle: Optional evidence bundle to add the evidence to
            
        Returns:
            Evidence ID if successful, None otherwise
        """
        # Get query and its most recent execution
        query = await self.get_query_provenance(query_id)
        if not query:
            logger.warning(f"Query {query_id} not found")
            return None
        
        executions = await self.get_query_executions(query_id)
        if not executions:
            logger.warning(f"No executions found for query {query_id}")
            return None
        
        # Get the most recent successful execution
        successful_executions = [e for e in executions if e.status == QueryStatus.SUCCESS]
        if not successful_executions:
            logger.warning(f"No successful executions found for query {query_id}")
            return None
        
        latest_execution = successful_executions[-1]
        
        # Create description if not provided
        if not description:
            description = f"Evidence from Cypher query: {query.description or query.query_text[:100]}"
        
        # Create evidence item
        evidence_item = GraphElementEvidence(
            description=description,
            source=source,
            confidence=confidence,
            raw_data={
                "query_id": query_id,
                "query_text": query.query_text,
                "parameters": query.parameters,
                "execution_id": latest_execution.execution_id,
                "result_summary": latest_execution.result_summary,
            },
            provenance_link=f"cypher:query:{query_id}",
            element_id=query_id,
            element_type=GraphElementType.SUBGRAPH,  # best fit for a query result
            element_properties={
                "query_text": query.query_text,
                "source": query.source.value,
                "timestamp": query.timestamp.isoformat(),
            },
        )
        
        # Add to bundle if provided
        if bundle:
            bundle.add_evidence(evidence_item)
        
        # Link query to evidence
        await self.link_query_to_evidence(query_id, evidence_item.id)
        
        return evidence_item.id

    async def search_queries(
        self,
        text_search: Optional[str] = None,
        source: Optional[QuerySource] = None,
        tags: Optional[List[str]] = None,
        from_timestamp: Optional[datetime] = None,
        to_timestamp: Optional[datetime] = None,
        limit: int = 10,
    ) -> List[CypherQuery]:
        """
        Searches for queries based on various criteria.
        
        Args:
            text_search: Optional text to search for in query text or description
            source: Optional source filter
            tags: Optional tags filter
            from_timestamp: Optional start timestamp
            to_timestamp: Optional end timestamp
            limit: Maximum number of results
            
        Returns:
            List of matching queries
        """
        # This is a simplified implementation
        # In a real implementation, we would use a more efficient search mechanism
        
        # Get all query keys
        pattern = "cypher:query:*"
        keys = self.redis_client.keys(pattern, self.query_provenance_db)
        
        # Fetch all queries
        queries = []
        for key in keys:
            data = self.redis_client.get(key, self.query_provenance_db, SerializationFormat.JSON)
            if data:
                query = CypherQuery(**data)
                queries.append(query)
        
        # Apply filters
        filtered_queries = []
        for query in queries:
            # Filter by text search
            if text_search and text_search.lower() not in query.query_text.lower() and (
                not query.description or text_search.lower() not in query.description.lower()
            ):
                continue
            
            # Filter by source
            if source and query.source != source:
                continue
            
            # Filter by tags
            if tags and not any(tag in query.tags for tag in tags):
                continue
            
            # Filter by timestamp range
            if from_timestamp and query.timestamp < from_timestamp:
                continue
            if to_timestamp and query.timestamp > to_timestamp:
                continue
            
            filtered_queries.append(query)
        
        # Sort by timestamp (newest first)
        filtered_queries.sort(key=lambda q: q.timestamp, reverse=True)
        
        # Apply limit
        return filtered_queries[:limit]

    async def get_query_metrics(self, query_id: str) -> Dict[str, Any]:
        """
        Gets performance metrics for a query.
        
        Args:
            query_id: Query ID
            
        Returns:
            Dictionary of metrics
        """
        executions = await self.get_query_executions(query_id)
        if not executions:
            return {
                "execution_count": 0,
                "avg_duration_ms": 0,
                "min_duration_ms": 0,
                "max_duration_ms": 0,
                "success_rate": 0,
                "cache_hit_rate": 0,
            }
        
        # Calculate metrics
        execution_count = len(executions)
        durations = [e.duration_ms for e in executions]
        avg_duration = sum(durations) / len(durations)
        min_duration = min(durations)
        max_duration = max(durations)
        
        success_count = sum(1 for e in executions if e.status == QueryStatus.SUCCESS)
        cache_hit_count = sum(1 for e in executions if e.status == QueryStatus.CACHED)
        
        success_rate = success_count / execution_count if execution_count > 0 else 0
        cache_hit_rate = cache_hit_count / execution_count if execution_count > 0 else 0
        
        return {
            "execution_count": execution_count,
            "avg_duration_ms": avg_duration,
            "min_duration_ms": min_duration,
            "max_duration_ms": max_duration,
            "success_rate": success_rate,
            "cache_hit_rate": cache_hit_rate,
            "last_execution_timestamp": executions[-1].timestamp.isoformat() if executions else None,
            "nodes_created": sum(e.nodes_created for e in executions),
            "relationships_created": sum(e.relationships_created for e in executions),
            "properties_set": sum(e.properties_set for e in executions),
            "labels_added": sum(e.labels_added for e in executions),
        }

    async def generate_query(
        self,
        natural_language_request: str,
        context: Optional[Dict[str, Any]] = None,
    ) -> Optional[CypherQuery]:
        """
        Generates a Cypher query from a natural language request.
        
        Args:
            natural_language_request: Natural language request
            context: Optional context for query generation
            
        Returns:
            Generated query or None if generation failed
        """
        # This is a placeholder for query generation
        # In a real implementation, this would use an LLM or similar
        
        logger.info(f"Query generation requested: {natural_language_request}")
        
        # For now, return a simple query based on keywords
        query_text = "MATCH (n) RETURN n LIMIT 10"  # Default fallback
        
        # Very simple keyword matching
        if "address" in natural_language_request.lower():
            if "transactions" in natural_language_request.lower():
                query_text = """
                MATCH (a:Address {address: $address})-[tx:TRANSFERRED]->(b:Address)
                RETURN a.address AS from_address, b.address AS to_address, tx.hash AS tx_hash, tx.value AS value, tx.timestamp AS timestamp
                ORDER BY tx.timestamp DESC
                LIMIT 10
                """
            else:
                query_text = """
                MATCH (a:Address {address: $address})
                RETURN a
                """
        
        elif "transaction" in natural_language_request.lower():
            query_text = """
            MATCH (a:Address)-[tx:TRANSFERRED {hash: $tx_hash}]->(b:Address)
            RETURN a.address AS from_address, b.address AS to_address, tx.value AS value, tx.timestamp AS timestamp
            """
        
        # Create query object
        query = CypherQuery(
            query_text=query_text,
            source=QuerySource.LLM_GENERATED,
            description=f"Generated from: {natural_language_request}",
            parameters={},  # Would be filled in a real implementation
        )
        
        # Store query
        await self._store_query_provenance(query)
        
        return query

    async def cite_query_in_response(
        self,
        query_id: str,
        response_text: str,
    ) -> str:
        """
        Adds a citation for a query to a response text.
        
        Args:
            query_id: Query ID
            response_text: Response text to add citation to
            
        Returns:
            Response text with citation
        """
        query = await self.get_query_provenance(query_id)
        if not query:
            return response_text
        
        # Create citation
        citation = f"\n\n[Query: {query_id}]\n```cypher\n{query.query_text}\n```"
        
        # Add parameters if present
        if query.parameters:
            params_str = json.dumps(query.parameters, indent=2)
            citation += f"\nParameters: ```json\n{params_str}\n```"
        
        # Add to response
        return response_text + citation

    async def get_query_citations(self, query_ids: List[str]) -> Dict[str, Dict[str, Any]]:
        """
        Gets citation information for multiple queries.
        
        Args:
            query_ids: List of query IDs
            
        Returns:
            Dictionary mapping query IDs to citation information
        """
        citations = {}
        
        for query_id in query_ids:
            query = await self.get_query_provenance(query_id)
            if not query:
                continue
            
            # Get the most recent execution
            executions = await self.get_query_executions(query_id)
            latest_execution = executions[-1] if executions else None
            
            # Create citation info
            citation_info = {
                "query_text": query.query_text,
                "parameters": query.parameters,
                "source": query.source.value,
                "timestamp": query.timestamp.isoformat(),
                "description": query.description,
                "execution_status": latest_execution.status.value if latest_execution else None,
                "execution_time_ms": latest_execution.duration_ms if latest_execution else None,
            }
            
            citations[query_id] = citation_info
        
        return citations
